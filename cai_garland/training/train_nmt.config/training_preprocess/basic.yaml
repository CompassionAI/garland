# Whether to shuffle the training dataset
shuffle_training_data:  true

# Number of random validation text entries to output to Tensorboard
eval_decodings_in_tensorboard: 20

# The number of processes to use for the preprocessing
preprocessing_num_workers: ~

# The maximum total sequence length for validation target text after tokenization. Sequences longer than this will be
#   truncated, sequences shorter will be padded. Will default to 'max_target_length'. This argument is also used to
#   override the max_length param of model.generate, which is used during 'evaluate' and 'predict'
val_max_target_length: ~

# Whether to pad all samples to model maximum sentence length. If false, will pad the samples dynamically when
#   batching to the maximum length in the batch. More efficient on GPU but very bad for TPU
pad_to_max_length: false

# Whether to ignore the tokens corresponding to padded labels in the loss computation or not
ignore_pad_token_for_loss: true

# For debugging purposes or quicker training, truncate the number of training examples to this value if set
max_train_samples: ~

# For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set
max_eval_samples: ~

# You can toggle padding quantization here if you like. It doesn't seem to make much difference
no_padding_quantization: false