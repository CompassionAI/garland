# Symbol cleanup mapping for the source language, relative to parallel_dataset_prep.py
symbol_cleaning_src_json: parallel_dataset_prep.config/symbol_cleaning_bo.json

# Symbol cleanup mapping for the target language, relative to parallel_dataset_prep.py
symbol_cleaning_tgt_json: parallel_dataset_prep.config/symbol_cleaning_en.json

# Check the English data for unknown tokens
check_for_en_unks: true

# Sort the parallel dataset by index of the English sentence in the parallel folio dataset
sort_by_starting_index: true

# Name of the tokenizer to use for packing data into registers. Note that the output is _not_ tokenized, this is only
#   used to count the final tokenized length of the target text
tokenizer_name: facebook/bart-base

# Maximum number of tokens output by the decoder (including bos and eos)
max_target_length: 1024

# Validation fraction to reserve
validation_frac: 0.0002

# Separator for datasets with multiple registers
separator: "[eor]"

# Postprocessing settings, after all stages are applied, right before outputting to disk.
#
# These are (possibly empty) lists of function classes in cai_garland.utils.str_processors, formatted like so:
#   - _target_: cai_garland.utils.str_processors.ProcessorSampleFunc1
#   - _target_: cai_garland.utils.str_processors.ProcessorSampleFunc2
#   - ...
postprocessing:
  source_lang:
    - _target_: cai_garland.utils.str_processors.ProcessorRemoveDanglingShads
    - _target_: cai_garland.utils.str_processors.ProcessorRemoveConsecutiveSpaces

  target_lang:
    - _target_: cai_garland.utils.str_processors.ProcessorLowerCase