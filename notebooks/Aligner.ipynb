{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de980207-16f1-4352-a9d2-acdd48cb5d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93ced5c1-8eaf-49d5-a616-498262c71cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/dask-worker-space\n",
    "! rm -rf ./dask-worker-space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c1393-03e0-494d-a17b-fc76d3578f9d",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0663730a-8b96-4c5b-9b93-75b32dfb757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import spacy\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import logging\n",
    "import unicodedata\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from cai_common.data import ParallelTMXLoader\n",
    "from dask.distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d07b288-2c26-4789-852b-93afd8c0cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_logger = logging.getLogger(\"distributed.utils_perf\")\n",
    "dask_logger.setLevel(logging.ERROR)\n",
    "\n",
    "dask_client = Client(LocalCluster(\n",
    "    n_workers=20,\n",
    "    threads_per_worker=1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e20ebc0-10f6-4182-9604-5f5d3c051a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>tohoku</th>\n",
       "      <th>folio</th>\n",
       "      <th>position</th>\n",
       "      <th>tibetan</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toh_384-Glorious_King_of_Tantras_That_Resolves...</td>\n",
       "      <td>384</td>\n",
       "      <td>F.187.a</td>\n",
       "      <td>1</td>\n",
       "      <td>དཔལ་རྡོ་རྗེ་སེམས་དཔའ་ལ་ཕྱག་འཚལ་ལོ། །</td>\n",
       "      <td>I pay homage to Glorious Vajrasattva!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toh_384-Glorious_King_of_Tantras_That_Resolves...</td>\n",
       "      <td>384</td>\n",
       "      <td>F.187.a</td>\n",
       "      <td>39</td>\n",
       "      <td>འདི་སྐད་བདག་གིས་ཐོས་པའི་དུས་གཅིག་ན། །</td>\n",
       "      <td>Thus have I heard at one time.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Toh_384-Glorious_King_of_Tantras_That_Resolves...</td>\n",
       "      <td>384</td>\n",
       "      <td>F.187.a</td>\n",
       "      <td>204</td>\n",
       "      <td>དེ་ནས་བྱང་ཆུབ་སེམས་དཔའ་རྡོ་རྗེ་སྙིང་པོ་ལ་སོགས་...</td>\n",
       "      <td>Then, the entourage, including bodhisattva Vaj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Toh_384-Glorious_King_of_Tantras_That_Resolves...</td>\n",
       "      <td>384</td>\n",
       "      <td>F.187.a</td>\n",
       "      <td>322</td>\n",
       "      <td>ཕྱི་ནང་གསང་བའི་མཆོད་པས་མཆོད་ནས་འདི་སྐད་ཅེས་གསོ...</td>\n",
       "      <td>made outer, inner, and secret offerings, and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toh_384-Glorious_King_of_Tantras_That_Resolves...</td>\n",
       "      <td>384</td>\n",
       "      <td>F.187.a</td>\n",
       "      <td>388</td>\n",
       "      <td>ཀྱེ་ཧོ་བཅོམ་ལྡན་རྡོ་རྗེ་འཛིན། །</td>\n",
       "      <td>O Blessed Vajra Holder!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Toh_309-The_Sutra_on_Impermanence-v1.tmx</td>\n",
       "      <td>309</td>\n",
       "      <td>F.155.b</td>\n",
       "      <td>650</td>\n",
       "      <td>ནད་མེད་མི་རྟག་ལང་ཚོ་རྟག་མ་ཡིན། །འབྱོར་པ་མི་རྟག...</td>\n",
       "      <td>“Good health is impermanent, Youth does not la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Toh_309-The_Sutra_on_Impermanence-v1.tmx</td>\n",
       "      <td>309</td>\n",
       "      <td>F.155.b</td>\n",
       "      <td>757</td>\n",
       "      <td>སྐྱེ་བོ་མི་རྟག་ཉིད་ཀྱིས་ཉེན་གྱུར་ན། །འདོད་པའི་...</td>\n",
       "      <td>How can beings, afflicted as they are by imper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Toh_309-The_Sutra_on_Impermanence-v1.tmx</td>\n",
       "      <td>309</td>\n",
       "      <td>F.155.b</td>\n",
       "      <td>858</td>\n",
       "      <td>བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་ནས། དགེ་ས...</td>\n",
       "      <td>When the Bhagavān had thus spoken, the monks r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Toh_309-The_Sutra_on_Impermanence-v1.tmx</td>\n",
       "      <td>309</td>\n",
       "      <td>F.155.b</td>\n",
       "      <td>935</td>\n",
       "      <td>མི་རྟག་པ་ཉིད་ཀྱི་མདོ་རྫོགས་སོ།། །།</td>\n",
       "      <td>This completes “The Sūtra on Impermanence.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Toh_309-The_Sutra_on_Impermanence-v1.tmx</td>\n",
       "      <td>309</td>\n",
       "      <td>F.155.b</td>\n",
       "      <td>977</td>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་སུ་རེན་དྲ་བོ་དྷི་དང་། ཞུ་ཆེན...</td>\n",
       "      <td>Translated and edited by the Indian scholar Su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>164496 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename tohoku    folio  \\\n",
       "0   Toh_384-Glorious_King_of_Tantras_That_Resolves...    384  F.187.a   \n",
       "1   Toh_384-Glorious_King_of_Tantras_That_Resolves...    384  F.187.a   \n",
       "2   Toh_384-Glorious_King_of_Tantras_That_Resolves...    384  F.187.a   \n",
       "3   Toh_384-Glorious_King_of_Tantras_That_Resolves...    384  F.187.a   \n",
       "4   Toh_384-Glorious_King_of_Tantras_That_Resolves...    384  F.187.a   \n",
       "..                                                ...    ...      ...   \n",
       "19           Toh_309-The_Sutra_on_Impermanence-v1.tmx    309  F.155.b   \n",
       "20           Toh_309-The_Sutra_on_Impermanence-v1.tmx    309  F.155.b   \n",
       "21           Toh_309-The_Sutra_on_Impermanence-v1.tmx    309  F.155.b   \n",
       "22           Toh_309-The_Sutra_on_Impermanence-v1.tmx    309  F.155.b   \n",
       "23           Toh_309-The_Sutra_on_Impermanence-v1.tmx    309  F.155.b   \n",
       "\n",
       "   position                                            tibetan  \\\n",
       "0         1               དཔལ་རྡོ་རྗེ་སེམས་དཔའ་ལ་ཕྱག་འཚལ་ལོ། །   \n",
       "1        39              འདི་སྐད་བདག་གིས་ཐོས་པའི་དུས་གཅིག་ན། །   \n",
       "2       204  དེ་ནས་བྱང་ཆུབ་སེམས་དཔའ་རྡོ་རྗེ་སྙིང་པོ་ལ་སོགས་...   \n",
       "3       322  ཕྱི་ནང་གསང་བའི་མཆོད་པས་མཆོད་ནས་འདི་སྐད་ཅེས་གསོ...   \n",
       "4       388                    ཀྱེ་ཧོ་བཅོམ་ལྡན་རྡོ་རྗེ་འཛིན། །   \n",
       "..      ...                                                ...   \n",
       "19      650  ནད་མེད་མི་རྟག་ལང་ཚོ་རྟག་མ་ཡིན། །འབྱོར་པ་མི་རྟག...   \n",
       "20      757  སྐྱེ་བོ་མི་རྟག་ཉིད་ཀྱིས་ཉེན་གྱུར་ན། །འདོད་པའི་...   \n",
       "21      858  བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་ནས། དགེ་ས...   \n",
       "22      935                 མི་རྟག་པ་ཉིད་ཀྱི་མདོ་རྫོགས་སོ།། །།   \n",
       "23      977  རྒྱ་གར་གྱི་མཁན་པོ་སུ་རེན་དྲ་བོ་དྷི་དང་། ཞུ་ཆེན...   \n",
       "\n",
       "                                              english  \n",
       "0               I pay homage to Glorious Vajrasattva!  \n",
       "1                      Thus have I heard at one time.  \n",
       "2   Then, the entourage, including bodhisattva Vaj...  \n",
       "3   made outer, inner, and secret offerings, and a...  \n",
       "4                             O Blessed Vajra Holder!  \n",
       "..                                                ...  \n",
       "19  “Good health is impermanent, Youth does not la...  \n",
       "20  How can beings, afflicted as they are by imper...  \n",
       "21  When the Bhagavān had thus spoken, the monks r...  \n",
       "22        This completes “The Sūtra on Impermanence.”  \n",
       "23  Translated and edited by the Indian scholar Su...  \n",
       "\n",
       "[164496 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_df = ParallelTMXLoader().apply_markup().clean_bad_chars().dataframe.compute()\n",
    "parallel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41224943-ca6f-4ded-9bc5-c2aca0db00b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87ce9938-2448-41e1-a6ee-8195cb680d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad46b9c-f05d-4c75-8f3b-d5bdc3a40f94",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95395253-31be-41fa-b75d-940fa59dbab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6f5a14-48c8-49d7-8b75-a6e209b41016",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = parallel_df.english.iloc[i]\n",
    "parsed = nlp(example)\n",
    "\n",
    "print(example)\n",
    "print()\n",
    "for token in parsed:\n",
    "    print(f\"{token.text:<20} {token.lemma_:<20} {token.pos_}\")\n",
    "\n",
    "i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da125834-341c-4c1f-864e-89228a2ebe61",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d2bc154-8bf8-41f5-b4c0-4d95eb7fcba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eligible_pos = {'VERB', 'NOUN', 'ADJ', 'PROPN'}\n",
    "num_positives = 3500\n",
    "negative_ratio = 3\n",
    "validation_frac = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08338146-c52d-4bfd-a290-38dbb21f7138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720cacad9a6342bba265d7dd2f98a779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positive_records = []\n",
    "\n",
    "records = random.choices(parallel_df.to_dict(orient=\"records\"), k=num_positives)\n",
    "final_allowed = set(string.ascii_lowercase)\n",
    "\n",
    "for record in tqdm(records):\n",
    "    parsed = nlp(record['english'])\n",
    "    positive_record = []\n",
    "    for token in filter(lambda token: token.pos_ in eligible_pos, parsed):\n",
    "        lemma = \"\".join([c for c in unicodedata.normalize('NFKD', token.lemma_) if not unicodedata.combining(c)]).lower()\n",
    "        lemma = ''.join([c for c in lemma if c in final_allowed])\n",
    "        if lemma == '':\n",
    "            continue\n",
    "        lemma = re.sub(r'\\s+', ' ', lemma).strip()\n",
    "        positive_record.append({\n",
    "            'source': record['tibetan'],\n",
    "            'target': lemma,\n",
    "            'pos': token.pos_,\n",
    "            'label': 1\n",
    "        })\n",
    "    positive_records.append(positive_record)\n",
    "\n",
    "positive_examples = [ex for rec in positive_records for ex in rec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ded7e33-1d97-4307-bf5e-0d2edef7f829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import NOUN, VERB, ADJECTIVE\n",
    "from pattern.en.wordnet import synsets\n",
    "\n",
    "pos_map = {\n",
    "    'NOUN': NOUN,\n",
    "    'PROPN': NOUN,\n",
    "    'VERB': VERB,\n",
    "    'ADJ': ADJECTIVE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4918d84-d90f-4588-9f9a-cb45c3ac3eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6823eaecdc2e40ccadf831951f3cc40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24386 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "by_tag = {tag: [] for tag in eligible_pos}\n",
    "\n",
    "for example in tqdm(positive_examples):\n",
    "    synonyms = synsets(example['target'], pos_map[example['pos']])\n",
    "    if len(synonyms) == 0:\n",
    "        by_tag[example['pos']].append([example['target'].lower()])\n",
    "    else:\n",
    "        all_synonyms = sorted(list(set([synonym.replace('_', ' ') for synonym in synonyms[0].synonyms])))\n",
    "        by_tag[example['pos']].append(all_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd67c248-2dc3-4f4b-9f0e-3bbb2d0e2569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dad1003f42a4c0ea14f8df1c77fb3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "negative_records = []\n",
    "\n",
    "for pos_record in tqdm(positive_records):\n",
    "    neg_record = []\n",
    "    for example in pos_record:\n",
    "        blacked_out_targets = {example['target']}\n",
    "        for _ in range(negative_ratio):\n",
    "            example = copy.deepcopy(example)\n",
    "            while True:\n",
    "                target = random.choice(\n",
    "                    random.choice(\n",
    "                        by_tag[example['pos']]\n",
    "                    )    # First choose a meaning\n",
    "                )        #     ...then choose a synonym\n",
    "                if not target in blacked_out_targets:\n",
    "                    break\n",
    "            example['target'] = target\n",
    "            example['label'] = 0\n",
    "            blacked_out_targets.add(target)\n",
    "            neg_record.append(example)\n",
    "    negative_records.append(neg_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d0bf9f9-4a26-4048-833f-3eea17a2177a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive part: 21971+2415 = 24386 = 24386'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train_size = int(len(positive_records) * (1 - validation_frac))\n",
    "\n",
    "pos_train_dataset = [ex for i in range(pos_train_size) for ex in positive_records[i]]\n",
    "pos_val_dataset = [ex for i in range(pos_train_size, len(positive_records)) for ex in positive_records[i]]\n",
    "f\"Positive part: {len(pos_train_dataset)}+{len(pos_val_dataset)} = {len(pos_train_dataset) + len(pos_val_dataset)} = {len(positive_examples)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30d93bc9-fd23-454d-93cf-db3cb5cfe29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative part: 65913+7245 = 73158 = 73158'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_len = sum([1 for rec in negative_records for ex in rec])\n",
    "neg_train_size = int(len(negative_records) * (1 - validation_frac))\n",
    "\n",
    "neg_train_dataset = [ex for i in range(neg_train_size) for ex in negative_records[i]]\n",
    "neg_val_dataset = [ex for i in range(neg_train_size, len(negative_records)) for ex in negative_records[i]]\n",
    "f\"Negative part: {len(neg_train_dataset)}+{len(neg_val_dataset)} = {len(neg_train_dataset) + len(neg_val_dataset)} = {neg_len}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd98907e-0297-4ec5-8514-05f7700a00aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/eeisenst/workspace/temp/temp_data/aligner_dataset'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_fn = os.path.join(os.environ['CAI_TEMP_PATH'], \"temp_data/aligner_dataset\")\n",
    "os.makedirs(dataset_fn, exist_ok=True)\n",
    "dataset_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14782c78-3581-4099-9c72-a55155877638",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pos_train_dataset + neg_train_dataset\n",
    "val_dataset = pos_val_dataset + neg_val_dataset\n",
    "\n",
    "pd.DataFrame(train_dataset).to_csv(os.path.join(dataset_fn, \"train.csv\"))\n",
    "pd.DataFrame(val_dataset).to_csv(os.path.join(dataset_fn, \"validation.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5489b086-51b5-4591-8b47-9dab7e15ab72",
   "metadata": {},
   "source": [
    "# Post training analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18586a6c-4dcf-479d-9074-f81a09dc19cf",
   "metadata": {},
   "source": [
    "## Load the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d573828b-689a-43de-8449-018c2720fb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cai_common.models.utils import get_local_ckpt, get_cai_config\n",
    "from cai_garland.models.cai_encoder_decoder_seq_class import CAIEncoderDecoderForSequenceClassification\n",
    "from cai_garland.models.factory import make_bilingual_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d11953-d135-4876-bd03-a562c7cc85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_ckpt = get_local_ckpt(\"experiments/aligner/olive-cormorant-bart\", model_dir=True)\n",
    "aligner = CAIEncoderDecoderForSequenceClassification.from_pretrained(local_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "667ae46e-1251-4a21-b5e7-0cbb1a275dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = aligner.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1ccef4c-33ce-4396-aec3-1ae3325fc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cai_base_config = get_cai_config(local_ckpt)\n",
    "encoder_name = cai_base_config['encoder_model_name']\n",
    "encoder_length = cai_base_config['encoder_max_length']\n",
    "decoder_name = cai_base_config['decoder_model_name']\n",
    "decoder_length = cai_base_config['decoder_max_length']\n",
    "\n",
    "tokenizer = make_bilingual_tokenizer(encoder_name, decoder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e9204c-cf82-42f3-91af-d8dd1736ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source, target, word, pos = '།སྐྱེ་བོ་ཐམས་ཅད་དགའ་བར་འགྱུར་རོ།', 'one will be loved by everyone.', 'love', 'VERB'\n",
    "# source, target, word, pos = '།སྐྱེ་བོ་ཐམས་ཅད་དགའ་བར་འགྱུར་རོ།', 'one will be loved by everyone.', 'everyone', 'NOUN'\n",
    "\n",
    "# source, target, word, pos = \"ནད་ཐམས་ཅད་སོས་པར་གྱུར་ཏེ།\", \"he will cure all diseases\", 'cure', 'VERB'\n",
    "# source, target, word, pos = \"ནད་ཐམས་ཅད་སོས་པར་གྱུར་ཏེ།\", \"he will cure all diseases\", 'disease', 'NOUN'\n",
    "\n",
    "source, target, word, pos = '།འཁྲུལ་འཁོར་ཐམས་ཅད་འགེམས་པར་བྱེད་དོ།', 'one will burn all the diagrams.', 'burn', 'VERB'\n",
    "# source, target, word, pos = '།འཁྲུལ་འཁོར་ཐམས་ཅད་འགེམས་པར་བྱེད་དོ།', 'one will burn all the diagrams.', 'diagram', 'NOUN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fc003dc-5d73-4bc0-977f-90ad6ba33ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eeisenst/workspace/transformers/src/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(source, return_tensors=\"pt\").to(aligner.device)\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    inputs_tgt = tokenizer(f\"{pos}{tokenizer.target_tokenizer.mask_token}{word}\", return_tensors=\"pt\").to(aligner.device)\n",
    "inputs[\"decoder_input_ids\"] = inputs_tgt[\"input_ids\"]\n",
    "inputs[\"decoder_attention_mask\"] = inputs_tgt[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064622b1-e8ce-49ff-99a4-65e82cc41821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(aligner(**inputs).logits.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fe29e1-c9a3-4a87-9bd4-b5a045bdaed1",
   "metadata": {},
   "source": [
    "## Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a9ce63-c232-43dc-8af6-71fec7424ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from captum.attr import TokenReferenceBase, visualization, LayerIntegratedGradients, LayerDeepLift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10919310-0ed9-4e05-9884-d05a39f0e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_viz_record(attribs, inputs, pred, pred_ind, label, delta, tokenizer):\n",
    "    attribs = attribs / torch.norm(attribs)\n",
    "    attribs = attribs.cpu().detach().numpy()\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        label = tokenizer.decode(label)\n",
    "    \n",
    "    return visualization.VisualizationDataRecord(\n",
    "        attribs,\n",
    "        0.5,\n",
    "        pred_ind,\n",
    "        label,\n",
    "        label,\n",
    "        attribs.sum(),\n",
    "        tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist()),\n",
    "        delta\n",
    "    )\n",
    "\n",
    "def lig_attribs(source, word, pos, aligner, tokenizer):\n",
    "    aligner.zero_grad()\n",
    "\n",
    "    inputs = tokenizer(source, return_tensors=\"pt\").to(aligner.device)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        inputs_tgt = tokenizer(f\"{pos}{tokenizer.target_tokenizer.mask_token}{word}\", return_tensors=\"pt\").to(aligner.device)\n",
    "    inputs[\"decoder_input_ids\"] = inputs_tgt[\"input_ids\"]\n",
    "    inputs[\"decoder_attention_mask\"] = inputs_tgt[\"attention_mask\"]\n",
    "    del inputs_tgt\n",
    "    \n",
    "    token_reference = TokenReferenceBase(reference_token_idx=tokenizer.source_tokenizer.pad_token_id)\n",
    "    seq_length = inputs['input_ids'].shape[1]\n",
    "    input_indices = inputs['input_ids']\n",
    "    reference_indices = token_reference.generate_reference(seq_length, device=aligner.device).unsqueeze(0)\n",
    "    reference_indices[0][0], reference_indices[0][-1] = tokenizer.bos_token_id, tokenizer.eos_token_id\n",
    "\n",
    "    def forward_func(input_ids):\n",
    "        new_inputs = {}\n",
    "        new_inputs['input_ids'] = input_ids\n",
    "        new_inputs['attention_mask'] = torch.ones_like(input_ids)\n",
    "        new_inputs['decoder_input_ids'] = inputs['decoder_input_ids'].repeat(input_ids.shape[0], 1)\n",
    "        new_inputs['decoder_attention_mask'] = inputs['decoder_attention_mask'].repeat(input_ids.shape[0], 1)\n",
    "        logits = aligner(**new_inputs).logits[:,1]#.unsqueeze(1)\n",
    "        # print(new_inputs)\n",
    "        # print(logits)\n",
    "        # print()\n",
    "        return logits\n",
    "\n",
    "    lig = LayerIntegratedGradients(forward_func, aligner.encoder.embeddings.word_embeddings)\n",
    "    igs, delta = lig.attribute(input_indices, reference_indices, n_steps=100, return_convergence_delta=True)\n",
    "    attribs = igs.sum(dim=2).squeeze(0)\n",
    "\n",
    "    certainty_score = attribs.detach().cpu().numpy()\n",
    "    certainty_score.sort()\n",
    "    certainty_score = abs(certainty_score[-1] / certainty_score[:-1].mean())\n",
    "    \n",
    "    return inputs, attribs, certainty_score, float(abs(delta[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f45ce001-9c5f-4a8d-a5d5-c8361dd14e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eeisenst/workspace/transformers/src/transformers/tokenization_utils_base.py:3542: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs, attribs, certainty_score, delta = lig_attribs(source, word, pos, aligner, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5949e4b3-fac7-4a0c-a3e1-c0756acbc17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000, -0.0943,  0.0506,  0.2051,  0.4350,  0.8224,  0.1388,  0.0669,\n",
       "          0.2731,  0.1122,  0.0000], device='cuda:0', dtype=torch.float64),\n",
       " 6.925712566676644,\n",
       " 0.06474727249194512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribs, certainty_score, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35c3ce09-fd96-40e1-9e7d-f557e84e4662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b><pad></b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.50)</b></text></td><td><text style=\"padding-right:2em\"><b><pad></b></text></td><td><text style=\"padding-right:2em\"><b>1.98</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> །                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> འཁྲུལ་འཁོར་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ཐམས་ཅད་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 60%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> འགེམས་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> པར་བྱེད་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ད                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ོ                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> །                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz_records = [make_viz_record(attribs, inputs, 1, 1, 1, delta, tokenizer)]\n",
    "_ = visualization.visualize_text(viz_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06178a56-a944-4f56-9af2-aa4f5f73242a",
   "metadata": {},
   "source": [
    "## Training bitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "765dfbbe-7c5b-4869-a90f-86a6e426dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61b9dc2b-6c9a-446b-a004-71e5240e97d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>འཇིག་རྟེན་འཛིན་བྱང་ཆུབ་སེམས་དཔའ་སེམས་དཔའ་ཆེན་པ...</td>\n",
       "      <td>lokadhara</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>འཇིག་རྟེན་འཛིན་བྱང་ཆུབ་སེམས་དཔའ་སེམས་དཔའ་ཆེན་པ...</td>\n",
       "      <td>bodhisattva</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>འཇིག་རྟེན་འཛིན་བྱང་ཆུབ་སེམས་དཔའ་སེམས་དཔའ་ཆེན་པ...</td>\n",
       "      <td>great</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>འཇིག་རྟེན་འཛིན་བྱང་ཆུབ་སེམས་དཔའ་སེམས་དཔའ་ཆེན་པ...</td>\n",
       "      <td>being</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>འཇིག་རྟེན་འཛིན་བྱང་ཆུབ་སེམས་དཔའ་སེམས་དཔའ་ཆེན་པ...</td>\n",
       "      <td>contemplate</td>\n",
       "      <td>VERB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9655</th>\n",
       "      <td>9655</td>\n",
       "      <td>གལ་ཏེ་ཡུལ་འདིར་གྲུ་གུ་ལ་སོགས་པ་གཏུམ་པོ་དད་པ་མ་...</td>\n",
       "      <td>unity</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9656</th>\n",
       "      <td>9656</td>\n",
       "      <td>གལ་ཏེ་ཡུལ་འདིར་གྲུ་གུ་ལ་སོགས་པ་གཏུམ་པོ་དད་པ་མ་...</td>\n",
       "      <td>sugata</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9657</th>\n",
       "      <td>9657</td>\n",
       "      <td>གལ་ཏེ་ཡུལ་འདིར་གྲུ་གུ་ལ་སོགས་པ་གཏུམ་པོ་དད་པ་མ་...</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9658</th>\n",
       "      <td>9658</td>\n",
       "      <td>གལ་ཏེ་ཡུལ་འདིར་གྲུ་གུ་ལ་སོགས་པ་གཏུམ་པོ་དད་པ་མ་...</td>\n",
       "      <td>respond</td>\n",
       "      <td>VERB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9659</th>\n",
       "      <td>9659</td>\n",
       "      <td>གལ་ཏེ་ཡུལ་འདིར་གྲུ་གུ་ལ་སོགས་པ་གཏུམ་པོ་དད་པ་མ་...</td>\n",
       "      <td>use</td>\n",
       "      <td>VERB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9660 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                             source  \\\n",
       "0              0  འཇིག་རྟེན་འཛིན་བྱང་ཆུབ་སེམས་དཔའ་སེམས་དཔའ་ཆེན་པ...   \n",
       "1              1  འཇིག་རྟེན་འཛིན་བྱང་ཆུབ་སེམས་དཔའ་སེམས་དཔའ་ཆེན་པ...   \n",
       "2              2  འཇིག་རྟེན་འཛིན་བྱང་ཆུབ་སེམས་དཔའ་སེམས་དཔའ་ཆེན་པ...   \n",
       "3              3  འཇིག་རྟེན་འཛིན་བྱང་ཆུབ་སེམས་དཔའ་སེམས་དཔའ་ཆེན་པ...   \n",
       "4              4  འཇིག་རྟེན་འཛིན་བྱང་ཆུབ་སེམས་དཔའ་སེམས་དཔའ་ཆེན་པ...   \n",
       "...          ...                                                ...   \n",
       "9655        9655  གལ་ཏེ་ཡུལ་འདིར་གྲུ་གུ་ལ་སོགས་པ་གཏུམ་པོ་དད་པ་མ་...   \n",
       "9656        9656  གལ་ཏེ་ཡུལ་འདིར་གྲུ་གུ་ལ་སོགས་པ་གཏུམ་པོ་དད་པ་མ་...   \n",
       "9657        9657  གལ་ཏེ་ཡུལ་འདིར་གྲུ་གུ་ལ་སོགས་པ་གཏུམ་པོ་དད་པ་མ་...   \n",
       "9658        9658  གལ་ཏེ་ཡུལ་འདིར་གྲུ་གུ་ལ་སོགས་པ་གཏུམ་པོ་དད་པ་མ་...   \n",
       "9659        9659  གལ་ཏེ་ཡུལ་འདིར་གྲུ་གུ་ལ་སོགས་པ་གཏུམ་པོ་དད་པ་མ་...   \n",
       "\n",
       "           target    pos  label  \n",
       "0       lokadhara  PROPN      1  \n",
       "1     bodhisattva  PROPN      1  \n",
       "2           great    ADJ      1  \n",
       "3           being   NOUN      1  \n",
       "4     contemplate   VERB      1  \n",
       "...           ...    ...    ...  \n",
       "9655        unity  PROPN      0  \n",
       "9656       sugata  PROPN      0  \n",
       "9657           be   VERB      0  \n",
       "9658      respond   VERB      0  \n",
       "9659          use   VERB      0  \n",
       "\n",
       "[9660 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(os.environ['CAI_DATA_BASE_PATH'], \"experiments/aligner/validation.csv\"))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "d8190118-e9b3-4883-954e-903acea44014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "མི་དམན་པ་བདག་དེ་བཞིན་གཤེགས་པ་བདུད་རྩིའི་ཐིགས་པའི་རྒྱལ་པོའི་དྲུང་དུ་སོང་སྟེ། དོན་འདི་ཡོངས་སུ་ཞུ་བར་བྱའོ། ། ask VERB\n"
     ]
    }
   ],
   "source": [
    "row = df[df.label==1].sample(n=1).iloc[0]\n",
    "print(row.source, row.target, row.pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e16e82aa-e3a7-4277-9e8c-c840637f91ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ཞུ་\n",
      "tensor([ 0.0000,  0.3236, -0.1151, -0.0703,  0.1324,  0.0529,  0.1171, -0.0954,\n",
      "         0.0089,  0.0051, -0.0166, -0.0530,  0.0338, -0.0448, -0.2757, -0.1540,\n",
      "        -0.0506, -0.0341,  0.2437,  0.0561,  0.3646,  0.1372,  1.4243, -0.0934,\n",
      "        -0.0045, -0.1467, -0.2210, -0.1689, -0.2439, -0.2170,  0.0000],\n",
      "       device='cuda:0', dtype=torch.float64) 80.68306842515587 1.5229128805359429\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b><pad></b></text></td><td><text style=\"padding-right:2em\"><b>1 (0.50)</b></text></td><td><text style=\"padding-right:2em\"><b><pad></b></text></td><td><text style=\"padding-right:2em\"><b>0.54</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁མི་                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> དམན་པ་                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> བདག་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> དེ་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> བཞིན་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> གཤེགས་པ་                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> བདུད་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> རྩིའི་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ཐིགས་                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> པའི་                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> རྒྱལ་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> པོའི་                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> དྲུང་དུ་                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> སོང་                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> སྟ                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ེ                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> །                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁དོན་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> འདི་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ཡོངས་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> སུ་                    </font></mark><mark style=\"background-color: hsl(120, 75%, 57%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ཞུ་                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> བར་                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> བྱ                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> འ                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ོ                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> །                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> །                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs, attribs, certainty_score, delta = lig_attribs(row.source, row.target, row.pos, aligner, tokenizer)\n",
    "\n",
    "print(int(aligner(**inputs).logits.argmax()))\n",
    "\n",
    "print(tokenizer.decode(inputs['input_ids'][0][attribs.argmax()]))\n",
    "# source_word = inputs['input_ids'][]\n",
    "\n",
    "print(attribs, certainty_score, delta)\n",
    "viz_records = [make_viz_record(attribs, inputs, 1, 1, 1, delta, tokenizer)]\n",
    "_ = visualization.visualize_text(viz_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603916d-d49c-48a8-88ea-969f66fb47bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
