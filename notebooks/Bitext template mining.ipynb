{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c17b1142-ede6-4f3b-8e3b-a5c7a5373e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "956f6072-ec30-41c0-a93c-d11e2bce866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fns = glob.glob(\"../../lotsawa/pos_tags/target_token_count/*.pkl\")\n",
    "fns = glob.glob(\"../../lotsawa/pos_tags/*.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "744ef373-a926-484a-97b2-0ada2668972c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d9d0549ab7647b7b0e1b1327ee45505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_sents = []\n",
    "for fn in tqdm(fns):\n",
    "    with open(fn, 'rb') as f:\n",
    "        all_sents.extend(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e114e59-5fec-4086-aa7a-765ca5730d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1038341"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5890c5d1-c0b4-4ef8-bde3-9aa038266e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d36a6715cc747a09871fcc6b8ab8530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1038341 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "910364"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_sent = {\n",
    "    re.sub(\"[A-Z\\[\\]]\", \"\", ''.join(words)).replace('།།', '། །').strip(): (words, tags) for words, tags in tqdm(all_sents)\n",
    "}\n",
    "len(by_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7056e004-d3f6-4cd7-97a8-c8dae67ad124",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4c4a24-4a9a-464f-aa79-997203393f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689c894-145d-44cf-ab36-ce32a4f5204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = sorted(list(set([tag for _, tags in all_sents for tag in tags])))\n",
    "len(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d9fd3-ff69-4a84-b4c3-5a9391a3e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_tag_weighted = {tag: [] for tag in all_tags}\n",
    "for words, tags in all_sents:\n",
    "    for word, tag in zip(words, tags):\n",
    "        by_tag_weighted[tag].append(word)\n",
    "\n",
    "by_tag = {word: sorted(list(set(tags))) for word, tags in by_tag_weighted.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba902aeb-c157-4188-b08e-80cb071586a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {tag: len(words) for tag, words in by_tag.items()}\n",
    "pd.Series(counts).sort_values().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c09d4-4a78-4bdc-9502-7472dade29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_len = {len_: [] for len_ in set([len(words) for words, _ in all_sents])}\n",
    "for words, tags in all_sents:\n",
    "    by_len[len(words)].append((words, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f5923-d011-4d93-af72-081f042dbf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series({len_: len(examples) for len_, examples in by_len.items()}).plot(kind='bar', figsize=(20,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02508e7-a2c8-4886-bd0a-e1d74ef91e72",
   "metadata": {},
   "source": [
    "# Explore possible templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9a6de5-66b2-4bfc-ae0d-b9e55e0d7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f2f2c9-77f8-4c15-9baa-9e159e2656dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_pos = by_len[7]\n",
    "len(cand_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24590b32-11dc-4d75-9250-1a7b6c02840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb20075-8927-4785-82eb-10b05a4936f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "for words, tags in cand_pos:\n",
    "    template = '-'.join(tags)\n",
    "    templates[template] = templates.get(template, []) + ['-'.join(words)]\n",
    "\n",
    "for template, words in templates.items():\n",
    "    templates[template] = [w.split('-') for w in sorted(list(set(words)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686f8a5b-db56-4264-b7fb-7ec7dd3f256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_len = pd.Series({template: len(words) for template, words in templates.items()}).sort_values()\n",
    "by_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f00708-0edd-4e36-beeb-ebc0b54e1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_len[by_len > 50].describe(percentiles=np.arange(0.9, 1.001, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62457142-e8a3-4fce-a7a2-dc8c2f79b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pick = by_len.index[-4]\n",
    "to_pick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c422b652-7804-434e-8f98-045fd328c145",
   "metadata": {},
   "source": [
    "# Test out sentence surgery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c478ab-9500-4ce6-a9e5-e4d8b445cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/dask-worker-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208c1a50-56d3-45df-b781-9af0a99f066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "dask_logger = logging.getLogger(\"distributed.utils_perf\")\n",
    "dask_logger.setLevel(logging.ERROR)\n",
    "\n",
    "dask_client = Client(LocalCluster(\n",
    "    n_workers=20,\n",
    "    threads_per_worker=1\n",
    "))\n",
    "\n",
    "# dask_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a7a7b9-a83c-4cf0-ae61-48c18fc49374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cai_common.dict import TibetanDict\n",
    "\n",
    "dict_ = TibetanDict(glob_override=\"processed_datasets/tibetan-english-dictionaries-for-aug/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13450cd6-a9c0-43b0-b3dd-5da59610caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "\n",
    "from cai_garland.utils.translator import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5034416-92df-406f-8e44-c8c59f4cbecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(\"olive-cormorant-nllb/base-600M\")\n",
    "\n",
    "translator.num_beams = 50\n",
    "translator.decoding_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe18da6-d6ee-4186-9594-7f60259a8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# words, tags = random.choice(all_sents)\n",
    "# words, tags = random.choice(by_len[7])\n",
    "tags = to_pick.split('-')\n",
    "words = random.choice(templates[to_pick])\n",
    "sent = re.sub(\"[A-Z\\[\\]]\", \"\", ''.join(words)).replace('།།', '། །').strip()\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f282d-2af0-4655-b9a0-eacd0d35a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.translate(sent, generator_kwargs={\"repetition_penalty\": 2.2, \"no_repeat_ngram_size\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70af1f23-0c81-4280-a275-f4b654dea812",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([f\"{word}-{tag}\" for word, tag in zip(words, tags)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75b191-0c89-43b4-b609-d24165398aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.decoding_length = 10\n",
    "for to_replace in tqdm(words[1:-1]):\n",
    "    print(to_replace)\n",
    "    print(dict_[to_replace])\n",
    "    print(translator.translate(\n",
    "        to_replace,\n",
    "        generator_kwargs={\"repetition_penalty\": 2.2, \"no_repeat_ngram_size\": 3}\n",
    "    ))\n",
    "    print()\n",
    "translator.decoding_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d2d15e-9144-48fe-8f5a-f91693325772",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace = \"ནད་\"\n",
    "to_replace, tags[words.index(to_replace)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244608c-2057-4160-abd3-77e26230247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_word = random.choice(by_tag_weighted[tags[words.index(to_replace)]])\n",
    "print(random_word)\n",
    "dict_[random_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd8309-13c7-4c6f-a7e7-77d6f7c6ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "changed_words = copy.deepcopy(words)\n",
    "changed_words[changed_words.index(to_replace)] = random_word\n",
    "changed_sent = re.sub(\"[A-Z\\[\\]]\", \"\", ''.join(changed_words)).replace('།།', '། །').strip()\n",
    "changed_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb9daae-f06a-4efd-a3f9-13ab3dd4a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator.translate(\n",
    "    changed_sent,\n",
    "    generator_kwargs={\n",
    "        \"repetition_penalty\": 2.2,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        # \"force_words_ids\": [meaning_tokens]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187ba94-a12e-4783-87d7-fd9afbf104f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meaning = \"play\"\n",
    "# with translator.tokenizer.as_target_tokenizer():\n",
    "#     meaning_tokens = translator.tokenizer.encode(meaning, add_special_tokens=False)\n",
    "#     print(meaning_tokens, translator.tokenizer.decode(meaning_tokens), meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4863d-a710-49a3-9269-1e390a2d0219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ca80da0-1c43-441b-b5f0-42b64a8bb1d4",
   "metadata": {},
   "source": [
    "# Lexer experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a367c-997b-40bb-9e38-19e1b16f7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab492b7-9727-4e3b-b067-344b58e21f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = nlp('he will cure all illnesses.')\n",
    "\n",
    "# for token in doc:\n",
    "#     print(token)\n",
    "#     print(token.morph)\n",
    "#     print(token.morph.get(\"PronType\"))\n",
    "#     print(token.lemma_)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823d97e-d689-41be-9bb1-0a3cb58021bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68857e8f-3882-4574-85be-0b1666e5bd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import conjugate, lemma, lexeme, PRESENT, SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf68ebc-3c9b-4f8c-b147-8e21cfc48eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\")\n",
    "model = RobertaForMaskedLM.from_pretrained(\"roberta-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada437b5-926c-4776-beee-e4751ce478af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"he will cure all <mask><mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559a5a9-1ec4-4d19-bbb5-b9e98f51ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = tokenizer(sent, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69740d76-db52-43c9-af3b-a931ef0a57e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(**input_).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64337d1-0914-4d16-9bbb-7471e12c292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token_index = (input_.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "tokenizer.decode(predicted_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b249168d-e35a-4dcd-a075-457b4d079467",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \" fraction\"\n",
    "lexemes = lexeme(word)\n",
    "lexemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c93c0-d874-4cb5-ab26-0c672744492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = tokenizer.batch_encode_plus(lexeme(word), add_special_tokens=False)['input_ids']\n",
    "candidates, max(map(len, candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7653126-1cf3-408b-bcda-56f12c6b0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import log_softmax, softmax\n",
    "\n",
    "scores = []\n",
    "\n",
    "mask_idxs = (input_['input_ids'][0] == tokenizer.mask_token_id).nonzero().squeeze().tolist()\n",
    "for idx, cand_ts in enumerate(candidates):\n",
    "    cur_scores = []\n",
    "    for cand_t, mask_i in zip(cand_ts, mask_idxs):\n",
    "        cur_scores.append(float(logits[0][mask_i][cand_t]))\n",
    "    print(cur_scores)\n",
    "    scores.append(sum(cur_scores) / len(cur_scores))\n",
    "    print()\n",
    "scores = np.array(scores)\n",
    "# scores = log_softmax(scores)\n",
    "\n",
    "scores, lexemes[scores.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dfcfd9-1264-4c0b-9dec-e27886387cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(logits.flatten()).hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a4d321-4bc8-4a00-be91-9001ac98c185",
   "metadata": {},
   "source": [
    "# Captum experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36957865-c301-417f-95ca-ebbca8a101ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/dask-worker-space\n",
    "! rm -rf /home/eeisenst/workspace/compassionai/garland/notebooks/dask-worker-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95719613-9c0f-4ec4-bcc9-5c2c78cfff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from captum.attr import LayerIntegratedGradients, TokenReferenceBase, visualization\n",
    "from cai_garland.utils.translator import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca73850-0590-4b41-901f-f0c6a60b9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "dask_logger = logging.getLogger(\"distributed.utils_perf\")\n",
    "dask_logger.setLevel(logging.ERROR)\n",
    "\n",
    "dask_client = Client(LocalCluster(\n",
    "    n_workers=20,\n",
    "    threads_per_worker=1\n",
    "))\n",
    "\n",
    "# dask_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2f77721-3c13-4f0a-abd0-878af2dc4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cai_common.dict import TibetanDict\n",
    "\n",
    "dict_ = TibetanDict(glob_override=\"processed_datasets/tibetan-english-dictionaries-for-aug/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76558a8a-0325-4752-8c25-b9f2d8a5c85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'NllbTokenizer'. \n",
      "The class this function is called from is 'CAINllbTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "translator = Translator(\"olive-cormorant-nllb/base-600M\")\n",
    "\n",
    "translator.num_beams = 50\n",
    "translator.decoding_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7372a86f-7194-4044-9fbe-347f995def40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_attribs = []\n",
    "\n",
    "def make_viz_record(attribs, inputs, pred, pred_ind, label, delta, tokenizer):\n",
    "    attribs = attribs / torch.norm(attribs)\n",
    "    attribs = attribs.cpu().detach().numpy()\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        label = tokenizer.decode(label)\n",
    "    \n",
    "    return visualization.VisualizationDataRecord(\n",
    "        attribs,\n",
    "        0.5,\n",
    "        pred_ind,\n",
    "        label,\n",
    "        label,\n",
    "        attribs.sum(),\n",
    "        tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].tolist()),\n",
    "        delta\n",
    "    )\n",
    "\n",
    "def lig_attribs(source, target, next_target, translator):\n",
    "    translator.model.zero_grad()\n",
    "\n",
    "    inputs = translator.tokenizer(source, return_tensors=\"pt\")\n",
    "    with translator.tokenizer.as_target_tokenizer():\n",
    "        _ = translator.tokenizer(target, return_tensors=\"pt\")\n",
    "        inputs['decoder_input_ids'] = _['input_ids']\n",
    "        inputs['decoder_attention_mask'] = _['attention_mask']\n",
    "        del _\n",
    "    with translator.tokenizer.as_target_tokenizer():\n",
    "        label = translator.tokenizer(next_target, return_tensors=\"pt\")['input_ids']\n",
    "    label = label[0, 2:-1]\n",
    "\n",
    "    pad_index = translator.tokenizer.pad_token_id\n",
    "    token_reference = TokenReferenceBase(reference_token_idx=pad_index)\n",
    "    seq_length = inputs['input_ids'].shape[1]\n",
    "    input_indices = inputs['input_ids']\n",
    "    device = translator.model.device\n",
    "    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n",
    "    reference_indices[0][0], reference_indices[0][-1] = translator.tokenizer.bos_token_id, translator.tokenizer.eos_token_id\n",
    "\n",
    "    def forward_func(input_ids):\n",
    "        new_inputs = {}\n",
    "        new_inputs['input_ids'] = input_ids\n",
    "        new_inputs['attention_mask'] = torch.ones_like(input_ids)\n",
    "        new_inputs['decoder_input_ids'] = inputs['decoder_input_ids'].repeat(input_ids.shape[0], 1)\n",
    "        new_inputs['decoder_attention_mask'] = inputs['decoder_attention_mask'].repeat(input_ids.shape[0], 1)\n",
    "        logits = translator.model(**new_inputs).logits[0][-2]\n",
    "        return logits\n",
    "\n",
    "    lig = LayerIntegratedGradients(forward_func, translator.model.encoder.embeddings.word_embeddings, multiply_by_inputs=False)\n",
    "\n",
    "    igs, delta = lig.attribute(input_indices, reference_indices, n_steps=500, return_convergence_delta=True)\n",
    "    attribs = igs.sum(dim=2).squeeze(0)\n",
    "\n",
    "    return inputs, label, attribs, sum([abs(delta[l]) for l in label]) / len(label)\n",
    "\n",
    "def attrib_all_words(source, target, translator):\n",
    "    target_words = target.split(' ')\n",
    "    viz_records = []\n",
    "    for w_idx, word in tqdm(enumerate(target_words), total=len(target_words)):\n",
    "        cur_target = ' '.join(target_words[:w_idx + 1])\n",
    "        inputs, label, attribs, delta = lig_attribs(source, cur_target, word, translator)\n",
    "        if inputs is None:\n",
    "            continue\n",
    "        all_attribs.append(attribs.tolist())\n",
    "        viz_records.append(make_viz_record(attribs, inputs, cur_target, -1, label, delta, translator.tokenizer))\n",
    "    _ = visualization.visualize_text(viz_records)\n",
    "    return viz_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3913915c-845a-4b23-b996-372d8b255710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source, target = \"ནད་ཐམས་ཅད་སོས་པར་གྱུར་ཏེ།\", \"he will cure all diseases\"\n",
    "# source, target = '།སྐྱེ་བོ་ཐམས་ཅད་དགའ་བར་འགྱུར་རོ།', 'one will be loved by everyone.'\n",
    "source, target = '།འཁྲུལ་འཁོར་ཐམས་ཅད་འགེམས་པར་བྱེད་དོ།', 'one will burn all the diagrams.'\n",
    "\n",
    "# viz_records = attrib_all_words(source, target, translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3860cd4-3d70-4051-8e89-39afcc2cc548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-[CLS] 1-▁ 2-། 3-འཁྲུལ་འཁོར་ 4-ཐམས་ཅད་ 5-འགེམས་ 6-པར་བྱེད་ 7-ད 8-ོ 9-། 10-[SEP]\n",
      "one will burn all the diagrams.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    ' '.join([\n",
    "        f\"{i}-{t}\" for i, t in enumerate(translator.tokenizer.convert_ids_to_tokens(translator.tokenizer.encode(source)))\n",
    "    ])\n",
    ")\n",
    "print(target)\n",
    "\n",
    "# all_attribs = np.vstack(all_attribs)\n",
    "# adj_attribs = (all_attribs - all_attribs.mean(axis=0))\n",
    "# for attribs in all_attribs:\n",
    "#     print(' '.join([f\"{att:>8.3f}\" for att in attribs]))\n",
    "# print(\"===\")\n",
    "# for attribs in adj_attribs:\n",
    "#     print(' '.join([f\"{att:>8.3f}\" for att in attribs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "480164df-7ce2-4b65-8301-6c2ff2ac4d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329051d0f8564d86a06c4c19733f0b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "འཁྲུལ་འཁོར་\n",
      "['(lit. machine) yogic exercises', \"1) [adhisara] [yogic] exercise, [hatha yoga]. 2) [yantra] magical wheel. 3) vicious cycle, wheel, cycle of confusion, samsaric confusion, deceptive round, vicious circle, deceptive round, 4) gadgetry, mechanism, machinery, 5) black magic. Syn {mig 'khrul} Syn {'phrul 'khor} 6) craft, artifice\", 'esoteric yogic practice', 'magical wheel', 'mechanism', 'Yogic practices. Exercises utilized in the Six Doctrines of Naropa', 'yantra', 'yantra', 'yantra', 'yantra']\n",
      "an illusion,\n",
      "\n",
      "ཐམས་ཅད་\n",
      "['everyone', 'the entire', 'whole, all, everything, everybody', 'everyone']\n",
      "all of them,\n",
      "\n",
      "འགེམས་\n",
      "[\"Syn {'joms}\"]\n",
      "one will be arrested\n",
      "\n",
      "པར་བྱེད་\n",
      "None\n",
      "promote it,\n",
      "\n",
      "དོ\n",
      "[\"1) sentence ending particle [used after the final 'd']. 2) match, counterpart, equal, island two, a pair, a couple\"]\n",
      "tha.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words, tags = by_sent[source]\n",
    "\n",
    "translator.decoding_length = 10\n",
    "for to_replace in tqdm(words[1:-1]):\n",
    "    print(to_replace)\n",
    "    print(dict_[to_replace])\n",
    "    print(translator.translate(\n",
    "        to_replace,\n",
    "        generator_kwargs={\"repetition_penalty\": 2.2, \"no_repeat_ngram_size\": 3}\n",
    "    ))\n",
    "    print()\n",
    "translator.decoding_length = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf49e1-6b95-45f1-8518-003167d35d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_viz_records = []\n",
    "for viz_i, viz_rec in enumerate(viz_records):\n",
    "    attribs = torch.tensor(adj_attribs)[viz_i]\n",
    "    attribs = attribs / torch.norm(attribs)\n",
    "    attribs = attribs.cpu().detach().numpy()\n",
    "    \n",
    "    new_viz_records.append(visualization.VisualizationDataRecord(\n",
    "        attribs,\n",
    "        0.5,\n",
    "        viz_rec.true_class,\n",
    "        viz_rec.true_class,\n",
    "        viz_rec.true_class,\n",
    "        attribs.sum(),\n",
    "        translator.tokenizer.convert_ids_to_tokens(translator.tokenizer.encode(source)),\n",
    "        0.5\n",
    "    ))\n",
    "_ = visualization.visualize_text(new_viz_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1cf33e-7d44-4d2b-ae48-14989237bf36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554058d-effd-46ed-9e37-4f40edbbc833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cdfe2a-9074-452f-a065-7fc08eabe07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cf96f5-4f51-4cca-9e5e-b6d51a488538",
   "metadata": {},
   "outputs": [],
   "source": [
    "source, target, next_target = '།སྐྱེ་བོ་ཐམས་ཅད་དགའ་བར་འགྱུར་རོ།', 'one will be loved by ', 'everyone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874bee7-227a-498d-95d3-6ce6581f0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = translator.tokenizer(source, return_tensors=\"pt\")\n",
    "with translator.tokenizer.as_target_tokenizer():\n",
    "    _ = translator.tokenizer(target, return_tensors=\"pt\")\n",
    "    inputs['decoder_input_ids'] = _['input_ids']#[:,:-1]\n",
    "    inputs['decoder_attention_mask'] = _['attention_mask']#[:,:-1]\n",
    "    del _\n",
    "# with translator.tokenizer.as_target_tokenizer():\n",
    "#     label = translator.tokenizer(next_target, return_tensors=\"pt\")['input_ids']\n",
    "# label = label[0, 2:-1]\n",
    "\n",
    "new_inputs = {}\n",
    "new_inputs['input_ids'] = inputs['input_ids']\n",
    "new_inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
    "new_inputs['decoder_input_ids'] = inputs['decoder_input_ids'].repeat(inputs['input_ids'].shape[0], 1)[:,:-1]\n",
    "new_inputs['decoder_attention_mask'] = inputs['decoder_attention_mask'].repeat(inputs['input_ids'].shape[0], 1)[:,:-1]\n",
    "logits = translator.model(**new_inputs).logits#[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd336af6-b315-4048-bd40-5e1a987604f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_inputs['decoder_input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2778a0e0-4d9b-4052-982a-550c6658e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with translator.tokenizer.as_target_tokenizer():\n",
    "    i = -3\n",
    "    print(translator.tokenizer.decode([inputs['decoder_input_ids'][0][i]]))\n",
    "    print(translator.tokenizer.batch_decode(logits[0][i].topk(50).indices.unsqueeze(-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
